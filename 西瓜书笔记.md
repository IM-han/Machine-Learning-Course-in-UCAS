<a name="content">目录</a>

[西瓜书笔记](#title)
- [1. 绪论](#1-introduction)
- [2. 线性模型](#2-linear-model)
	- [2.1. 线性回归](#2-linear-regression)
	- [2.2. logistic回归](#2-logistic-regression)
	- [2.3. 线性判别分析](#2-lda)
	- [2.4. 多分类学习](#2-multi-classfication-task)
		- [2.4.1. 一对一(OvO)](#2-one-vs-one)
		- [2.4.2. 一对其余(OvR)](#2-one-vs-rest)
		- [2.4.3. 多对多(MvM)](#2-many-vs-many)
- [3. 决策树](#3-decision-tree)
	- [3.1. 基本流程](#3-basic-steps)
	- [3.2. 划分选择](#3-partation-feature-chose)
		- [3.2.1. 信息熵 (information entropy)](#3-information-entropy)
		- [3.2.2. ID3：信息增益 (information gain)](#3-information-gain)
		- [3.2.3. ID4.5：增益率](#3-gain-ratio)
		- [3.2.4. CART：基尼指数](#3-gini-index)
			- [CART分类树建立算法的具体流程](#process-to-construct-cart-tree)
			- [CART回归树](#cart-regression-tree)
	- [3.3. 剪枝处理](#3-pruning)
		- [3.3.1. 预剪枝](#3-prepruning)
		- [3.3.2. 后剪枝](#3-post-pruning)
		- [3.3.3. CART决策树的剪枝](#3-pruning-for-cart-tree)
	- [3.4. 连续与缺失值](#3-continuous-and-missing-value)
		- [3.4.1. 连续值处理](#3-continuous-value)
		- [3.4.2. 缺失值处理](#3-missing-value)
			- [3.4.2.1. 问题(1)划分属性选择](#3-issue1-for-missing-value)
			- [3.4.2.2. 问题(2)属性值缺失样本的划分](#3-issue2-for-missing-value)
- [4. 支持向量机](#4-svm)
	- [4.1. 支持向量机与优化目标](#4-svm-optimization-function)
	- [4.2. 用对偶问题解SVM优化目标](#4-sove-optimazation-by-dual-problem)
		- [4.2.1. 对偶问题](#4-svm-dual-problem)
		- [4.2.2. 用SMO法解对偶问题](#4-solve-dual-problem-by-smo)
		- [4.2.3. 基于对偶问题的解得到最终解](#4-svm-get-final-values)
	- [4.3. 解决非线性可分问题：核函数](#4-solve-unlinear-problem-by-kernal)
		- [4.3.1. 非线性可分问题及解决策略：高维映射](#4-unlinear-problem-and-strategy)
		- [4.3.2. 解决高维映射带来的计算问题：构造核函数](#4-solve-hyperdimension-mapping-by-kernal)
			- [4.3.2.1. 核函数实现高效计算的例子](#4-example-of-kernal)
		- [4.3.3. 如何构造核函数](#4-how-to-construct-a-kernal)
	- [4.4. 软间隔与正则化](#4-soft-margin-and-regulation)
		- [4.4.1. 引入软间隔](#4-introduce-soft-margin)
		- [4.4.2. 原始0/1损失函数与替代损失函数](#4-loss-functions)
		- [4.4.3. 基于hinge损失的常用“软间隔SVM”](#4-most-common-soft-margin-svm)
		- [4.4.4. 不同软间隔SVM的共性](#4-commons-for-different-soft-margin-svm)
	- [4.5. 支持向量回归（SVR）](#4-svr)
		- [4.5.1. SVR的基本思想](#4-principle-of-svr)
		- [4.5.2. SVR的优化目标与求解](#4-optimization-for-svr)
- [5. 贝叶斯分类器](#5-bayes)
	- [5.1. 贝叶斯决策论](#5-bayes-classify-theory)
	- [5.2. 最小化分类错误率](#5-minimize-error-rate)
	- [5.3. 如何获得后验概率P(c|x)？](#5-how-to-get-posterior-probability)
		- [5.3.1. 判别式模型与生成式模型](#5-discriminative-models-and-generative-models)
		- [5.3.2. 估计类条件概率：极大似然估计](#5-maximum-likelihood-estimation)
	- [5.4. 朴素贝叶斯分类器](#5-naive-bayes-classifier)
		- [5.4.1. 什么是朴素贝叶斯分类器？](#5-what-is-naive-bayes-classifier)
		- [5.4.2. 如何训练？](#5-how-to-train-nbc)
		- [5.4.3. 数据平滑：Laplacian修正](#5-data-smoothing)
	- [5.5. 半朴素贝叶斯分类器](#5-semi-naive-bayes-classifier)
	- [5.6. 贝叶斯网](#5-bayesian-network)
		- [5.6.1. 结构](#5-bayesian-network-structure)
		- [5.6.2. 学习](#5-bayesian-network-learning)
		- [5.6.3. 推断：吉布斯采样](#5-bayesian-network-inference)
	- [5.7. EM算法](#5-em-algorithmn)
- [6. 神经网络](#6-neural-network)
	- [6.1. 神经元模型](#6-neuron-model)
	- [6.2. 感知机和多次网络](#6-perceptron-and-multilayer-network)
	- [6.3. 神经网络学习：BP算法](#6-neural-network-training)
		- [6.3.1. 标准BP算法](#6-standard-bp-algorithmn)
		- [6.3.2. 累积BP算法](#6-accumulated-error-backpropagation)
		- [6.3.3. 过拟合与解决方案](#6-overfitting-and-solving-methods)
	- [6.4. 全局最小与全局极小](#6-global-minimum-and-local-minimum)
- [番外一：一小时打完一次比赛](#extern1-one-hour-a-competation)
- [番外二：sklearn包中logistic回归算法的使用](#extern2-use-logistic-regression-in-sklearn)
	- [*2.1. 与logistic回归相关的类](#extern2-relative-class)
	- [*2.2. 优化算法](#extern2-optimize-algorithmn)
	- [*2.3. 正则化参数的选择：penalty](#extern2-regeneralization-parameter)
	- [*2.4. 分类方式选择参数：multi-class](#extern2-classify-method)



<h1 name="title">西瓜书笔记</h1>

<a name="1-introduction"><h2>1. 绪论 [<sup>目录</sup>](#content)</h2></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-01.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-02.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-03.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-04.jpg width=800 /></p>

<a name="2-linear-model"><h2>2. 线性模型 [<sup>目录</sup>](#content)</h2></a>

<a name="2-linear-regression"><h3>2.1. 线性回归 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-08.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-09_1.jpg width=800 /></p>

<a name="2-logistic-regression"><h3>2.2. logistic回归 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-09_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-10.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-11.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-12_1.jpg width=800 /></p>

<a name="2-lda"><h3>2.3. 线性判别分析 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-12_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-13.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-14.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-15.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-16_1.jpg width=800 /></p>

<a name="2-multi-classfication-task"><h3>2.4. 多分类学习 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-16_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-17_1.jpg width=800 /></p>

<a name="2-one-vs-one"><h4>2.4.1. 一对一(OvO) [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-17_2.jpg width=800 /></p>

<a name="2-one-vs-rest"><h4>2.4.2. 一对其余(OvR) [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-17_3.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-18_1.jpg width=800 /></p>

<a name="2-many-vs-many"><h4>2.4.3. 多对多(MvM) [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-18_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-19.jpg width=800 /></p>

<a name="3-decision-tree"><h2>3. 决策树 [<sup>目录</sup>](#content)</h2></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-2
4.jpg width=800 /></p>

<a name="3-basic-steps"><h3>3.1. 基本流程 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-25.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-26_1.jpg width=800 /></p>

<a name="3-partation-feature-chose"><h3>3.2. 划分选择 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-26_2.jpg width=800 /></p>

<a name="3-information-entropy"><h4>3.2.1. 信息熵 (information entropy) [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-26_3.jpg width=800 /></p>

Ent(D)的值越小，则D的纯度越高

<a name="3-information-gain"><h4>3.2.2. 信息增益 (information gain) [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-27.jpg width=800 /></p>

ID3算法的缺点：

> - (1) ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途
> 
> - (2) ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大
> 
> - (3) ID3算法对于缺失值的情况没有做考虑
> 
> - (4) 没有考虑过拟合的问题

针对各个问题，发展出来了对应的处理策略：

> - 对应(1)，[连续值的处理](#3-continuous-value)：使用连续属性离散化技术——二分法
> 
> - 对应(2)，[增益率](#3-gain-ratio)
> 
> - 对应(3)，[缺失值处理](#3-missing-value)
> 
> - 对应(4)，[剪枝处理](#3-pruning)

<a name="3-gain-ratio"><h4>3.2.3. 增益率 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-28.jpg width=800 /></p>

C4.5虽然改进或者改善了ID3算法的几个主要的问题，仍然有优化的空间

> - 1) 由于决策树算法非常容易过拟合，因此对于生成的决策树必须要进行剪枝。剪枝的算法有非常多，C4.5的剪枝方法有优化的空间。
> 
> - 2) C4.5生成的是多叉树，即一个父节点可以有多个节点。很多时候，在计算机中二叉树模型会比多叉树运算效率高。如果采用二叉树，可以提高效率。
> 
> - 3) C4.5只能用于分类，如果能将决策树用于回归的话可以扩大它的使用范围
> 
> - 4) C4.5由于使用了熵模型，里面有大量的耗时的对数运算,如果是连续值还有大量的排序运算。如果能够加以模型简化可以减少运算强度但又不牺牲太多准确性的话，那就更好了

这4个问题在CART树里面部分加以了改进

<a name="3-gini-index"><h4>3.2.4. 基尼指数 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-29.jpg width=800 /></p>

<a name="process-to-construct-cart-tree"><h4>CART分类树建立算法的具体流程 [<sup>目录</sup>](#content)</h4></a>

算法输入是训练集D，基尼系数的阈值，样本个数阈值

- 1) 对于当前节点的数据集为D，如果样本个数小于阈值或者没有特征，则返回决策子树，当前节点停止递归。

- 2) 计算样本集D的基尼系数，如果基尼系数小于阈值，则返回决策树子树，当前节点停止递归。

- 3) 计算当前节点现有的各个特征的各个特征值对数据集D的基尼系数，对于离散值和连续值的处理方法和基尼系数的计算见第二节。缺失值的处理方法和上篇的C4.5算法里描述的相同。

- 4) 在计算出来的各个特征的各个特征值对数据集D的基尼系数中，选择基尼系数最小的特征A和对应的特征值a。根据这个最优特征和最优特征值，把数据集划分成两部分D1和D2，同时建立当前节点的左右节点，做节点的数据集D为D1，右节点的数据集D为D2.

- 5) 对左右的子节点递归的调用1-4步，生成决策树。

<a name="cart-regression-tree"><h4>CART回归树 [<sup>目录</sup>](#content)</h4></a>

CART决策树与回归树的区别：

> 两者的区别在于样本输出，如果样本输出是离散值，那么这是一颗分类树。如果果样本输出是连续值，那么那么这是一颗回归树。
> 
> 除了概念的不同，CART回归树和CART分类树的建立和预测的区别主要有下面两点：
> 
> 1) 连续值的处理方法不同
> 
> 2) 决策树建立后做预测的方式不同

- **连续值的处理**

CART分类树采用的是用基尼系数的大小来度量特征的各个划分点的优劣情况。这比较适合分类模型

但是对于回归模型，我们使用了常见的和方差的度量方式，CART回归树的度量目标是，对于任意划分特征A，对应的任意划分点s两边划分成的数据集D1和D2，求出使**D1和D2各自集合的均方差最小**，同时**D1和D2的均方差之和最小**所对应的特征和特征值划分点

<p align="center"><img src=./picture/MachineLearning-ZZH-CART-regression-tree.png width=500 /></p>

- **预测的方式**

对于决策树建立后做预测的方式，上面讲到了CART分类树采用叶子节点里概率最大的类别作为当前节点的预测类别。而回归树输出不是类别，它采用的是用最终叶子的均值或者中位数来预测输出结果

<a name="3-pruning"><h3>3.3. 剪枝处理 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-30.jpg width=800 /></p>

<a name="3-prepruning"><h4>3.3.1. 预剪枝 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-31.jpg width=800 /></p>

<a name="3-post-pruning"><h4>3.3.2. 后剪枝 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-32.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-33_1.jpg width=800 /></p>

<a name="3-pruning-for-cart-tree"><h4>3.3.3. CART决策树的剪枝 [<sup>目录</sup>](#content)</h4></a>

对CART树进行剪枝，类似于线性回归的正则化，来增加决策树的泛化能力

CART采用的办法是后剪枝法，即先生成决策树，然后产生所有可能的剪枝后的CART树，然后使用交叉验证来检验各种剪枝的效果，选择泛化能力最好的剪枝策略

首先，我们看看**剪枝的损失函数度量**

> 在剪枝的过程中，对于任意的一刻子树T,其损失函数为：
> 
> <p align="center">C<sub>α</sub>(T<sub>t</sub>)=C(T<sub>t</sub>)+α|T<sub>t</sub>|</p>
> 
> 其中，α为正则化参数，这和线性回归的正则化一样。C(Tt)为训练数据的预测误差，分类树是用基尼系数度量，回归树是均方差度量。|Tt|是子树T的叶子节点的数量
> 
> 当α=0时，即没有正则化，原始的生成的CART树即为最优子树。当α=∞时，即正则化强度达到最大，此时由原始的生成的CART树的根节点组成的单节点树为最优子树。当然，这是两种极端情况。一般来说，α越大，则剪枝剪的越厉害，生成的最优子树相比原生决策树就越偏小。对于固定的α，一定存在使损失函数Cα(T)最小的唯一子树

**剪枝的思路**

> 对于位于节点t的任意一颗子树Tt，如果没有剪枝，它的损失是
> 
> <p align="center">C<sub>α</sub>(T<sub>t</sub>)=C(T<sub>t</sub>)+α|T<sub>t</sub>|</p>
> 
> 如果将其剪掉，仅仅保留根节点，则损失是
> 
> <p align="center">C<sub>α</sub>(T)=C(T)+α</p>
> 
> 当α=0或者α很小时，Cα(Tt)<Cα(T) , 当α增大到一定的程度时
> 
> <p align="center">C<sub>α</sub>(T<sub>t</sub>)=C<sub>α</sub>(T)</p>
> 
> 当α继续增大时不等式反向，也就是说，如果满足下式：
> 
> <p align="center"><img src=./picture/MachineLearning-ZZH-pruning-for-CART-tree.png height=70 /></p>
> 
> Tt和T有相同的损失函数，但是T节点更少，因此可以对子树Tt进行剪枝，也就是将它的子节点全部剪掉，变为一个叶子节点T

最后，我们看看**CART树的交叉验证策略**

> 可以计算出每个子树是否剪枝的阈值α，如果我们把所有的节点是否剪枝的值α都计算出来
> 
> 然后分别针对不同的α所对应的剪枝后的最优子树做交叉验证。这样就可以选择一个最好的α，有了这个α，我们就可以用对应的最优子树作为最终结果。

**CART树的剪枝算法过程**

> 输入是CART树建立算法得到的原始决策树T
> 
> 输出是最优决策子树Tα
> 
> - 1) 初始化α<sub>min</sub>=∞， 最优子树集合ω={T}
> 
> - 2) 从叶子节点开始自下而上计算各内部节点 t 的训练误差损失函数 C<sub>α</sub>(T<sub>t</sub>)（回归树为均方差，分类树为基尼系数）， 叶子节点数|Tt|，以及正则化阈值α=min{C(T)−C(Tt)|Tt|−1,αmin}， 更新α<sub>min</sub>=α
> 
> - 3) 得到所有节点的α值的集合M
> 
> - 4) 从M中选择最大的值αk，自上而下的访问子树t的内部节点，如果C(T)−C(Tt)/|Tt|−1≤αk时，进行剪枝。并决定叶节点t的值。如果是分类树，则是概率最高的类别，如果是回归树，则是所有样本输出的均值。这样得到αk对应的最优子树Tk
> 
> - 5) 最优子树集合ω=ω∪Tk， M=M−{αk}
> 
> - 6) 如果M不为空，则回到步骤4。否则就已经得到了所有的可选最优子树集合ω
> 
> - 7) 采用交叉验证在ω选择最优子树Tα

<a name="3-continuous-and-missing-value"><h3>3.4. 连续与缺失值 [<sup>目录</sup>](#content)</h3></a>

<a name="3-continuous-value"><h4>3.4.1. 连续值处理 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-33_2.jpg width=800 /></p>

<a name="3-missing-value"><h4>3.4.2. 缺失值处理 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-34_1.jpg width=800 /></p>

<a name="3-issue1-for-missing-value"><h4>3.4.2.1. 问题(1)划分属性选择 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-34_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-35_1.jpg width=800 /></p>

<a name="3-issue2-for-missing-value"><h4>3.4.2.2. 问题(2)属性值缺失样本的划分 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-35_2.jpg width=800 /></p>

<a name="4-svm"><h2>4. 支持向量机 [<sup>目录</sup>](#content)</h2></a>

<a name="4-svm-optimization-function"><h3>4.1. 支持向量机与优化目标 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-36.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-37.jpg width=800 /></p>

<a name="4-sove-optimazation-by-dual-problem"><h3>4.2. 用对偶问题解SVM优化目标 [<sup>目录</sup>](#content)</h3></a>

<a name="4-dual-problem"><h4>4.2.1. 对偶问题 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-38.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-39_1.jpg width=800 /></p>

<a name="4-solve-dual-problem-by-smo"><h4>4.2.2. 用SMO法解对偶问题 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-40.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-41.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-SMO-supplement.png width=800 /></p>

<a name="4-svm-get-final-values"><h4>4.2.3. 基于对偶问题的解得到最终解 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-39_2.jpg width=800 /></p>

假设我们有S个支持向量，则对应我们求出S个b<sup>∗</sup>,理论上这些b<sup>∗</sup>都可以作为最终的结果， 但是我们一般采用一种更健壮的办法，即求出所有支持向量所对应的b<sup>∗</sup><sub>s</sub>，然后将其平均值作为最后的结果

<a name="4-solve-unlinear-problem-by-kernal"><h3>4.3. 解决非线性可分问题：核函数 [<sup>目录</sup>](#content)</h3></a>

<a name="4-unlinear-problem-and-strategy"><h4>4.3.1. 非线性可分问题及解决策略：高维映射 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-42.jpg width=800 /></p>

<a name="4-solve-hyperdimension-mapping-by-kernal"><h4>4.3.2. 解决高维映射带来的计算问题：构造核函数 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-43_1.jpg width=800 /></p>

<a name="4-example-of-kernal"><h5>4.3.2.1. 核函数实现高效计算的例子 [<sup>目录</sup>](#content)</h5></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-44_1.jpg width=800 /></p>

<a name="4-how-to-construct-a-kernal"><h4>4.3.3. 如何构造核函数 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-43_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-44_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-45.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-46_1.jpg width=800 /></p>

<a name="4-soft-margin-and-regulation"><h3>4.4. 软间隔与正则化 [<sup>目录</sup>](#content)</h3></a>

<a name="4-introduce-soft-margin"><h4>4.4.1. 引入软间隔 [<sup>目录</sup>](#content)</h4></a>

有时候本来数据的确是可分的，也就是说可以用 线性分类SVM的学习方法来求解，但是却因为混入了**异常点**，导致不能线性可分，比如下图，本来数据是可以按下面的实线来做超平面分离的，可以由于一个橙色和一个蓝色的异常点导致我们没法按照线性支持向量机中的方法来分类

<p align="center"><img src=./picture/MachineLearning-ZZH-introduce-soft-margin-svm-1.png width=500 /></p>

另外一种情况没有这么糟糕到不可分，但是会严重影响我们模型的泛化预测效果，比如下图，本来如果我们不考虑异常点，SVM的超平面应该是下图中的红色线所示，但是由于有一个蓝色的异常点，导致我们学习到的超平面是下图中的粗虚线所示，这样会严重影响我们的分类模型预测效果。

<p align="center"><img src=./picture/MachineLearning-ZZH-introduce-soft-margin-svm-2.png width=500 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-46_2.jpg width=800 /></p>

<a name="4-loss-functions"><h4>4.4.2. 原始0/1损失函数与替代损失函数 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-47_1.jpg width=800 /></p>

<a name="4-most-common-soft-margin-svm"><h4>4.4.3. 基于hinge损失的常用“软间隔SVM” [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-47_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-48.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-49.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-50_1.jpg width=800 /></p>

<a name="4-commons-for-different-soft-margin-svm"><h4>4.4.4. 不同软间隔SVM的共性 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-50_2.jpg width=800 /></p>

<a name="4-svr"><h3>4.5. 支持向量回归（SVR） [<sup>目录</sup>](#content)</h3></a>

<a name="4-principle-of-svr"><h4>4.5.1. SVR的基本思想 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-51_1.jpg width=800 /></p>

<a name="4-optimization-for-svr"><h4>4.5.2. SVR的优化目标与求解 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-51_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-52.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-53.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-54.jpg width=800 /></p>

<a name="5-bayes"><h2>5. 贝叶斯分类器 [<sup>目录</sup>](#content)</h2></a>

<a name="5-bayes-classify-theory"><h3>5.1. 贝叶斯决策论 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-56.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-57_1.jpg width=800 /></p>

<a name="5-minimize-error-rate"><h3>5.2. 最小化分类错误率 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-57_2.jpg width=800 /></p>

<a name="5-how-to-get-posterior-probability"><h3>5.3. 如何获得后验概率P(c|x)？ [<sup>目录</sup>](#content)</h3></a>

<a name="5-discriminative-models-and-generative-models"><h4>5.3.1. 判别式模型与生成式模型 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-57_3.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-58.jpg width=800 /></p>

<a name="5-maximum-likelihood-estimation"><h4>5.3.2. 估计类条件概率：极大似然估计 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-59.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-60.jpg width=800 /></p>

<a name="5-naive-bayes-classifier"><h3>5.4. 朴素贝叶斯分类器 [<sup>目录</sup>](#content)</h3></a>

<a name="5-what-is-naive-bayes-classifier"><h4>5.4.1. 什么是朴素贝叶斯分类器？ [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-61_1.jpg width=800 /></p>

<a name="5-how-to-train-nbc"><h4>5.4.2. 如何训练？ [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-61_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-62_1.jpg width=800 /></p>

<a name="5-data-smoothing"><h4>5.4.3. 数据平滑：Laplacian修正 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-62_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-63_1.jpg width=800 /></p>

<a name="5-semi-naive-bayes-classifier"><h3>5.5. 半朴素贝叶斯分类器 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-63_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-64.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-65.jpg width=800 /></p>

<a name="5-bayesian-network"><h3>5.6. 贝叶斯网 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-66.jpg width=800 /></p>

<a name="5-bayesian-network-structure"><h4>5.6.1. 结构 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-67.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-68.jpg width=800 /></p>

<a name="5-bayesian-network-learning"><h4>5.6.2. 学习 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-69.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-70.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-71_1.jpg width=800 /></p>

<a name="5-bayesian-network-inference"><h4>5.6.3. 推断：吉布斯采样 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-71_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-72.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-73.jpg width=800 /></p>

<a name="5-em-algorithmn"><h3>5.7. EM算法 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-74.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-75.jpg width=800 /></p>

<a name="6-neural-network"><h2>6. 神经网络 [<sup>目录</sup>](#content)</h2></a>

<a name="6-neuron-model"><h3>6.1. 神经元模型 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-76.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-77_1.jpg width=800 /></p>

<a name="6-perceptron-and-multilayer-network"><h3>6.2. 感知机和多次网络 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-77_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-78.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-79.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-80.jpg width=800 /></p>

<a name="6-neural-network-training"><h3>6.3. 神经网络学习：BP算法 [<sup>目录</sup>](#content)</h3></a>

BP算法（error BackPropagation， BP)，误差反向传播算法

<a name="6-standard-bp-algorithmn"><h4>6.3.1. 标准BP算法 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-81.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-82.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-83.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-84.jpg width=800 /></p>

<a name="6-accumulated-error-backpropagation"><h4>6.3.2. 累积BP算法 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-85.jpg width=800 /></p>

<a name="6-overfitting-and-solving-methods"><h4>6.3.3. 过拟合与解决方案 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-86.jpg width=800 /></p>

<a name="6-global-minimum-and-local-minimum"><h3>6.4. 全局最小与全局极小 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-87.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-88.jpg width=800 /></p>


---

参考资料：

(1) 周志华《机器学习》

(2) [刘建平Pinard系列博客](https://www.cnblogs.com/pinard/)
