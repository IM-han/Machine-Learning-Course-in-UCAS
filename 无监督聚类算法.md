<a name="content">目录</a>

[无监督聚类方法](#title)
- [1. 谱聚类](#spectral-clustering)
	- [1.1. 一堆基础概念](#basic-knowledge-of-matrix)
	- [1.2. 拉普拉斯矩阵](#laplacian-matrix)
	- [1.3. 拉普拉斯矩阵的性质](#characters-of-laplacian-matrix)
	- [1.4. 谱聚类](#details-of-spectral-clustering)
		- [1.4.1. 算法思想及优化目标](#basic-ideal-of-algorithmn-and-optimaization-objective)
		- [1.4.2. 构造Laplacian图](#construct-laplacian-graph)
		- [1.4.3. 用Cut/Ratio Cut分割子图](#cut-graph-by-ratio-cut)
			- [1.4.3.1. 优化目标](#cut-graph-by-ratio-cut-optimization-objective)
			- [1.4.3.2. 求解优化问题：最小化RatioCut与最小化 f'Lf 等价](#cut-graph-by-ratio-cut-solve-method)
- [补充知识](#supplementary-knowledge)
	- [*1. 向量的乘法运算](#vector-multiplication)
		- [*1.1. 向量内积（点积）](#vector-dot-product)
		- [*1.2. 向量内积的用途](#app-of-vector-dot-product)
		- [*1.3. 向量外积](#vector-outer-product)
		- [*1.4. 向量外积的用途](#app-of-vector-outer-product)





<h1 name="title">无监督聚类方法</h1>

<a name="spectral-clustering"><h2>1. 谱聚类 [<sup>目录</sup>](#content)</h2></a>

<a name="basic-knowledge-of-matrix"><h3>1.1. 一堆基础概念 [<sup>目录</sup>](#content)</h3></a>

- **单位矩阵**

	在矩阵中，n阶单位矩阵，是一个 nxn 的方形矩阵，其主对角线元素为1，其余元素为0。单位矩阵以 I<sub>n</sub> 表示；如果阶数可忽略，或可由前后文确定的话，也可简记为I（或者E）。 如下图所示，便是一些单位矩阵：

	<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-Matrix-base-knowledge-1.png width=600 /></p>

	单位矩阵中的第i列即为单位向量v<sub>i</sub>，单位向量同时也是单位矩阵的特征向量，特征值皆为1，因此这是唯一的特征值，且具有重数n。由此可见，单位矩阵的行列式为1，且迹数为n。 

- **单位向量**

	向量空间中的单位向量就是长度为 1 的向量

	两个单位向量的点积就是它们之间角度的余弦（因为它们的长度都是 1）：

	<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-Matrix-base-knowledge-2.png width=600 /></p>

	补充知识——[向量的乘法运算](#vector-multiplication)

	一个非零向量u<sup>→</sup>的正规化向量（即单位向量）u<sup>^</sup>就是平行于u<sup>→</sup>的单位向量，记作：

	<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-Matrix-base-knowledge-3.png height=70 /></p>

	这里||u<sup>→</sup>||是u<sup>→</sup>的范数（长度）

- **正交与正交矩阵**

	正交是垂直这一直观概念的推广，若内积空间中两向量的内积（即点积）为0，则称它们是正交的，相当于这两向量垂直，换言之，如果能够定义向量间的夹角，则正交可以直观的理解为垂直

	**正交矩阵**（orthogonal matrix）是一个元素为实数，而且行与列皆为正交的单位向量的方块矩阵（方块矩阵，或简称方阵，是行数及列数皆相同的矩阵）

<a name="laplacian-matrix"><h3>1.2. 拉普拉斯矩阵 [<sup>目录</sup>](#content)</h3></a>

拉普拉斯矩阵（Laplacian matrix)），也称为基尔霍夫矩阵，是表示图的一种矩阵

给定一个有n个顶点的图 G=(V,E)，其拉普拉斯矩阵被定义为 L = D-A，D其中为图的度矩阵，A为图的邻接矩阵

例如给定下图G=(V,E)

<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-Laplacian-matrix-1.jpg width=300 /></p>

把此“图”转换为邻接矩阵的形式，记为A（假设每条边的连接权重均为1）：

<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-Laplacian-matrix-2.png width=300 /></p>

把W的每一列元素加起来得到N个数，然后把它们放在对角线上（其它地方都是零），组成一个N×N的对角矩阵，记为度矩阵D，如下图所示

<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-Laplacian-matrix-3.png width=300 /></p>

根据拉普拉斯矩阵的定义L = D-A，可得拉普拉斯矩阵L 为：

<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-Laplacian-matrix-4.png width=300 /></p>

显然，拉普拉斯矩阵都是对称

<a name="characters-of-laplacian-matrix"><h3>1.3. 拉普拉斯矩阵的性质 [<sup>目录</sup>](#content)</h3></a>

介绍拉普拉斯矩阵的性质之前，首先定义两个概念，如下：

> 1. 对于邻接矩阵，定义图中A子图与B子图之间所有边的权值之和如下：
> 
> 	<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-characters-of-Laplacian-matrix-1.png height=70 /></p>
> 
> 	其中，定义 w <sub>ij</sub>为节点 i 到节点 j 的权值，如果两个节点不是相连的，权值为零
> 
> 2. 与某结点邻接的所有边的权值和定义为该顶点的度d，多个d 形成一个度矩阵（对角阵）
> 
> 	<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-characters-of-Laplacian-matrix-2.png height=70 /></p>

拉普拉斯矩阵L具有如下性质：

- L 是对称半正定矩阵；
- L \* v<sub>1</sub><sup>→</sup> = 0 \* v<sub>1</sub><sup>→</sup>，即 L 的最小特征值是 0，相应的特征向量是 v<sub>1</sub><sup>→</sup>

	证明： L \* v<sub>1</sub><sup>→</sup> = (D-W) \* v<sub>1</sub><sup>→</sup> = 0 = 0 \* v<sub>1</sub><sup>→</sup>

	> 特征值和特征向量的定义：
	> 
	> 若数字λ和非零向量v<sup>→</sup>满足
	> 
	> <p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-characters-of-Laplacian-matrix-3.png height=70 /></p>
	> 
	> 则v<sup>→</sup>为A的一个特征向量，λ是其对应的特征值

- L 有n个非负实特征值

	<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-characters-of-Laplacian-matrix-4.png height=60 /></p>	

- 对于任何一个属于实向量 f∈R<sup>n</sup>，有以下式子成立

	<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-characters-of-Laplacian-matrix-5.png height=70 /></p>	

	其中

	<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-characters-of-Laplacian-matrix-6.png height=150 /></p>

	下面，来证明下上述结论，如下：

	<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-characters-of-Laplacian-matrix-7.png height=300 /></p>

<a name="details-of-spectral-clustering"><h3>1.4. 谱聚类 [<sup>目录</sup>](#content)</h3></a>

<a name="basic-ideal-of-algorithmn-and-optimaization-objective"><h4>1.4.1. 算法思想及优化目标 [<sup>目录</sup>](#content)</h4></a>

谱聚类本质上就是**将聚类问题转换为图论问题**

从图论的角度来说，聚类的问题就相当于一个图的分割问题。即给定一个图G = (V, E)，顶点集V表示各个样本，带权的边表示各个样本之间的相似度，谱聚类的目的便是要找到一种合理的分割图的方法，使得分割后形成若干个子图，**连接不同子图的边的权重（相似度）尽可能低，同子图内的边的权重（相似度）尽可能高**。物以类聚，人以群分，相似的在一块儿，不相似的彼此远离。

至于如何把图的顶点集分割/切割为不相交的子图有多种办法，如：

> - cut/Ratio Cut
> 
> - Normalized Cut
> 
> - 不基于图，而是转换成SVD能解的问题

优化目标为：**让被割掉各边的权值和最小**

因为被砍掉的边的权值和越小，代表被它们连接的子图之间的相似度越小，隔得越远，而相似度低的子图正好可以从中一刀切断。

<a name="construct-laplacian-graph"><h4>1.4.2. 构造Laplacian图 [<sup>目录</sup>](#content)</h4></a>

（1）先要构造相似度矩阵，任意两个对象x<sub>i</sub>和x<sub>j</sub>，其相似度基于高斯核函数（也称径向基函数核）计算相似度定义为：

<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-construct-Laplacian-matrix-1.png height=100 /></p>

距离越大，代表其相似度越小

相似度矩阵就是Laplacian图的邻接矩阵W

根据邻接矩阵W可以得到度矩阵

<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-characters-of-Laplacian-matrix-2.png height=70 /></p>

最后Laplacian矩阵为L=D-W

（2）子图A的指示向量如下：

<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-construct-Laplacian-matrix-2.jpg height=100 /></p>

<a name="cut-graph-by-ratio-cut"><h4>1.4.3. 用Cut/Ratio Cut分割子图 [<sup>目录</sup>](#content)</h4></a>

<a name="cut-graph-by-ratio-cut-optimization-objective"><h5>1.4.3.1. 优化目标 [<sup>目录</sup>](#content)</h5></a>

如何切割才能得到最优的结果呢？

要把图片分割为几个区域（或若干个组），要求是分割所得的 Cut 值最小，相当于那些被切断的边的权值之和最小

设 A<sub>1</sub>, A<sub>2</sub>, ..., A<sub>k</sub> 为图的几个子集（它们没有交集） ，为了让分割的Cut 值最小，谱聚类便是要最小化下述目标函数： 

<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-cut-graph-1.png height=70 /></p>

但很多时候，最小化cut 通常会导致不好的分割。以分成2类为例，这个式子通常会将图分成了一个点和其余的n-1个点。如下图所示，很明显，最小化的smallest cut不是最好的cut，反而把{A、B、C、H}分为一边，{D、E、F、G}分为一边很可能就是最好的cut：

<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-cut-graph-2.jpg width=600 /></p>

为了让每个类都有合理的大小，目标函数尽量让 A<sub>1</sub>, A<sub>2</sub>, ..., A<sub>k</sub> 足够大。改进后的目标函数为：

<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-cut-graph-3.png height=70 /></p>

其中| A <sub>i</sub> |表示 A <sub>i</sub> 组中包含的顶点数目

或者也可以将优化目标定义为最小化Ncut：

<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-cut-graph-4.png height=70 /></p>

其中

<p align="center"><img src=/picture/Unsupervise-Clustering-Spectral-Clustering-cut-graph-5.png height=70 /></p>

<a name="cut-graph-by-ratio-cut-solve-method"><h5>1.4.3.2. 求解优化问题：最小化RatioCut与最小化 f'Lf 等价 [<sup>目录</sup>](#content)</h5></a>










<a name="supplementary-knowledge"><h2>补充知识 [<sup>目录</sup>](#content)</h2></a>

<a name="vector-multiplication"><h3>*1. 向量的乘法运算 [<sup>目录</sup>](#content)</h3></a>

<a name="dot-product"><h4>*1.1. 向量内积（点积） [<sup>目录</sup>](#content)</h4></a>


<p align="center"><img src=/picture/Unsupervise-Clustering-supplementary-knowledge-vector-multiplication-1.png width=600 /></p>

使用矩阵乘法并把（纵列）向量当作n×1 矩阵，点积还可以写为：

<p align="center"><img src=/picture/Unsupervise-Clustering-supplementary-knowledge-vector-multiplication-12.png height=50 /></p>

向量内积性质

<p align="center"><img src=/picture/Unsupervise-Clustering-supplementary-knowledge-vector-multiplication-2.png width=600 /></p>

<p align="center"><img src=/picture/Unsupervise-Clustering-supplementary-knowledge-vector-multiplication-3.png width=600 /></p>
	
向量内积的物理意义
	
<p align="center"><img src=/picture/Unsupervise-Clustering-supplementary-knowledge-vector-multiplication-4.png width=600 /></p>

向量内积的物理意义是，力通过位移做功

<a name="app-of-vector-dot-product"><h4>*1.2. 向量内积的用途 [<sup>目录</sup>](#content)</h4></a>

- **求两个非零向量的夹角 **

	<p align="center"><img src=/picture/Unsupervise-Clustering-supplementary-knowledge-vector-multiplication-5.png width=600 /></p>

- **判断两个非零向量是否垂直**

	<p align="center"><img src=/picture/Unsupervise-Clustering-supplementary-knowledge-vector-multiplication-6.png width=600 /></p>

	简单的对应坐标相乘再求和，结果为0就垂直，否则就不垂直

<a name="vector-outer-product"><h4>*1.3. 向量外积 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=/picture/Unsupervise-Clustering-supplementary-knowledge-vector-multiplication-7.png width=600 /></p>

通过坐标进行外积的直接计算比较复杂，写成行列式的形式，再展开，方便记忆

<p align="center"><img src=/picture/Unsupervise-Clustering-supplementary-knowledge-vector-multiplication-8.png width=600 /></p>

向量外积的性质

<p align="center"><img src=/picture/Unsupervise-Clustering-supplementary-knowledge-vector-multiplication-9.png width=600 /></p>

向量外积的几何意义

<p align="center"><img src=/picture/Unsupervise-Clustering-supplementary-knowledge-vector-multiplication-10.png width=600 /></p>

再除以2的话，就是以 a，b 为边的三角形的面积

<a name="app-of-vector-outer-product"><h4>*1.4. 向量外积的用途 [<sup>目录</sup>](#content)</h4></a>

**求与三角形面积相关的问题**

<p align="center"><img src=/picture/Unsupervise-Clustering-supplementary-knowledge-vector-multiplication-11.png width=600 /></p>










---

参考资料：

(1) [CSDN·珠穆拉玛峰《从拉普拉斯矩阵说到谱聚类》](https://blog.csdn.net/guoxinian/article/details/79532893)

(2) [CSDN·鹅城惊喜师爷《【线性代数】向量的乘法运算》](https://blog.csdn.net/baishuo8/article/details/80858692)

(3) [行者无疆兮.csdn【图论】拉普拉斯矩阵（Laplacian matrix）](https://blog.csdn.net/qq_30159015/article/details/83271065)
