<a name="content">目录</a>

[改善深层神经网络](#title)
- [摘要](#abstract)
- [第一周：深度学习的实用层面](#week1)
	- [1.1. 深度学习的训练](#training-of-deeplearning)
	- [1.2. 偏差与方差](#bias-and-variance)
		- [1.2.1. 调整偏差与方差](#adjustment-of-bias-and-variance)
	- [1.3. 正则化](#regularization)
		- [1.3.1. 正则化防止过拟合的原因](#reason-of-regularization-preventing-overfit)
		- [1.3.2. dropout正则化](#dropout-regularization)
		- [1.3.3. 理解dropout](#understand-dropout)
		- [1.3.4. 其他正则化方法](#another-way-to-regularization)
			- [1.3.4.1. 扩增训练数据](#agument-traning-set)
			- [1.3.4.2. Early stopping](#early-stopping)
	- [1.4. 加速训练](#speedup-traning)
		- [1.4.1. 正则化输入](#regularize-input)
		- [1.4.2. 梯度消失/爆炸](#gradient-vanish-or-explosion)
			- [1.4.2.1. 解决方法：权重矩阵初始化](#weight-initialization)
	- [1.5. 梯度检查](#grad-check)
		- [1.5.1. 梯度逼近](#gradient-approximation)
		- [1.5.2. 实施过程](#operating-program)
- [第二周：优化算法](#week2)
	- [2.1. Mini-batch梯度下降方法](#mini-batch-gradient-descent)
		- [2.1.1. 什么是Mini-batch](#what-is-mini-batch)
		- [2.1.2. 理解Mini-batch](#understand-mini-batch)
	- [2.2. 指数加权平均](#exponentially-weighted-averages)
		- [2.2.1. 说明是指数加权平均](#what-is-exponentially-weighted-averages)
		- [2.2.2. 理解指数加权平均](#understand-exponentially-weighted-averages)
		- [2.2.3. 实施指数加权平均及其偏差修正](#carryout-bias-correction)
	- [2.3. Momentum梯度下降法](#gradient-descent-with-momentum)
	- [2.4. RMSprp梯度下降法](#gradient-descent-with-emsprop)
	- [2.5. Adam优化算法: Momentum + RMSprp](#adam)
	- [2.6. 学习率衰减](#learning-rate-decay)
	- [2.7. 解决局部最优问题](#local-optima)
- [第三周：超参数调试、Batch正则化和程序框架](#week3)
	- [3.1. 超参数调试](#debug-hyperparameters)
		- [3.1.1. 搜索超参数空间](#search-hyperparameters-space)
		- [3.1.2. 为超参数选择合适的搜索尺牍（范围）](#chose-proper-scale)
		- [3.1.3. 超参数训练的两大策略](#hyperparameters-training-strategies)
	- [3.2. Batch正则化](#batch-normalization)
		- [3.2.1. 正则化网络的激活函数](#normalizing-activation-in-a-network)
		- [3.2.2. 实施Batch归一化](#implementing-batch-norm)
		- [3.2.3. 将Batch Norm 拟合进神经网络](#apply-batch-norm-in-nn)
		- [3.2.4. 理解Batch Norm的作用](#understand-batch-norm)
		- [3.2.5. Batch Norm在训练与测试中的不同操作](#batch-norm-during-train-or-test-time)
	- [3.3. softmax回归](#softmax-regression)


<h1 name="title">改善深层神经网络</h1>

<p align="center"><img src=./picture/deeplearning.ai.logo.png width="400" /></p>

<a name="abstract"><h3>摘要 [<sup>目录</sup>](#content)</h3></a>

<img src=./picture/Improving-DeepNeuralNetwork-summary-1.png width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-summary-2.png width="800" />

<a name="week1"><h3>第一周：深度学习的实用层面 [<sup>目录</sup>](#content)</h3></a>

<a name="training-of-deeplearning"><h4>1.1. 深度学习的训练 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-1.jpg width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week1-2-1.jpg width="800" />

<a name="bias-and-variance"><h4>1.2. 偏差与方差 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-bais-and-variance.png width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week1-2-2.jpg width="800" />

高偏差与高方差的例子：

<img src=./picture/Improving-DeepNeuralNetwork-2hign-example.png width="800" />

用紫色线画出的分类器具有高偏差和高方差，**高偏差**：它几乎是一条线性分类器，并未拟合数据；**高方差**：曲线中间部分灵活性非常高，却过度拟合了两个错误样本

<a name="adjustment-of-bias-and-variance"><h4>1.2.1. 调整偏差与方差 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-3.jpg width="800" />

<a name="regularization"><h4>1.3. 正则化 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-4.jpg width="800" />

<a name="reason-of-regularization-preventing-overfit"><h4>1.3.1. 正则化防止过拟合的原因 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-5-1.jpg width="800" />

<a name="dropout-regularization"><h4>1.3.2. dropout正则化 [<sup>目录</sup>](#content)</h4></a>

<table>
<tr>
	<td><img src=./picture/Improving-DeepNeuralNetwork-dropout-regularization-1.png width="400" /></td>
	<td><img src=./picture/Improving-DeepNeuralNetwork-dropout-regularization-2.png width="400" /></td>
</tr>
</table>

<img src=./picture/Improving-DeepNeuralNetwork-week1-5-2.jpg width="800" />

<a name="understand-dropout"><h4>1.3.3. 理解dropout [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-6-1.jpg width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week1-6-2.jpg width="800" />

<a name="another-way-to-regularization"><h4>1.3.4. 其他正则化方法 [<sup>目录</sup>](#content)</h4></a>

<a name="agument-traning-set"><h4>1.3.4.1. 扩增训练数据 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-7-1.jpg width="800" />

<a name="early-stopping"><h4>1.3.4.2. Early stopping [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-7-2.jpg width="800" />

<a name="speedup-traning"><h4>1.4. 加速训练  [<sup>目录</sup>](#content)</h4></a>

<a name="regularize-input"><h4>1.4.1. 加速训练方法一：正则化输入 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-7-3.jpg width="800" />

<table>
<tr>
	<td><img src=./picture/Improving-DeepNeuralNetwork-week2-unnormalized-1.png width="400" /></td>
	<td><img src=./picture/Improving-DeepNeuralNetwork-week2-normalized-1.png width="400" /></td>
</tr>
<tr>
	<td><img src=./picture/Improving-DeepNeuralNetwork-week2-unnormalized-2.png width="400" /></td>
	<td><img src=./picture/Improving-DeepNeuralNetwork-week2-normalized-2.png width="400" /></td>
</tr>
</table>

<img src=./picture/Improving-DeepNeuralNetwork-week1-8-1.jpg width="800" />

<a name="gradient-vanish-or-explosion"><h4>1.4.2. 梯度消失/爆炸 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-8-2.jpg width="800" />

<a name="weight-initialization"><h4>1.4.2.1. 解决方法：权重矩阵初始化 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-9-1.jpg width="800" />

<a name="grad-check"><h4>1.5. 梯度检查 [<sup>目录</sup>](#content)</h4></a>

<a name="gradient-approximation"><h4>1.5.1. 梯度逼近 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-9-2.jpg width="800" />

<a name="operating-program"><h4>1.5.2. 实施过程 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week1-10-1.jpg width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week1-10-2.jpg width="800" />

<a name="week2"><h3>第二周：优化算法 [<sup>目录</sup>](#content)</h3></a>

<a name="mini-batch-gradient-descent"><h4>2.1. Mini-batch梯度下降方法 [<sup>目录</sup>](#content)</h4></a>

<a name="what-is-mini-batch"><h4>2.1.1. 什么是Mini-batch [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-1.jpg width="800" />

<a name="understand-mini-batch"><h4>2.1.2. 理解Mini-batch [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-2-1.jpg width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week2-2-2.jpg width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week2-3-1.jpg width="800" />

<a name="exponentially-weighted-averages"><h4>2.2. 指数加权平均 [<sup>目录</sup>](#content)</h4></a>

<a name="what-is-exponentially-weighted-averages"><h4>2.2.1. 什么是指数加权平均 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-3-2.jpg width="800" />

<a name="understand-exponentially-weighted-averages"><h4>2.2.2. 理解指数加权平均 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-4-1.jpg width="800" />

<a name="carryout-bias-correction"><h4>2.2.3. 实施指数加权平均及其偏差修正 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-4-2.jpg width="500" />

<img src=./picture/Improving-DeepNeuralNetwork-week2-5-1.jpg width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week2-5-1-2.png width="800" />

<a name="gradient-descent-with-momentum"><h4>2.3. Momentum梯度下降法 [<sup>目录</sup>](#content)</h4></a>

SGD方法的一个缺点是其更新方向完全依赖于当前batch计算出的梯度，因而十分不稳定。Momentum算法借用了物理中的动量概念，它模拟的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力

<img src=./picture/Improving-DeepNeuralNetwork-week2-5-2.jpg width="800" />

<a name="gradient-descent-with-emsprop"><h4>2.4. RMSprp梯度下降法 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-6-2.png width="800" />

对于上图所示的梯度下降情景，我们希望对于震荡比较大的方向，即b方向，减缓它的梯度下降的速度，而对于于震荡比较小的方向，即w方向，加快它的梯度下降的速度

总而言之，就通过震荡程度来缩放当前的梯度下降速度，震荡程度与缩放程度成反比，即若震荡越强，则缩放程度越小（减缓），若震荡越弱，则缩放程度越大（加快）

那么如何实现呢？

如果可以定义出一个统计量来定量描述某一方向上的震荡程度，则可以将当前计算出来的实际梯度除以这个统计量，使得满足反比的要求

我们很容易能想到，可以用统计学中的方差或标准差来定量描述这种震荡程度

<img src=./picture/Improving-DeepNeuralNetwork-week2-6.jpg width="800" />

<a name="adam"><h4>2.5. Adam优化算法: Momentum + RMSprp [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-7-1.jpg width="800" />

<a name="learning-rate-decay"><h4>2.6. 学习率衰减 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/Improving-DeepNeuralNetwork-week2-leaning-rate-decay.png width="500" /></p>

<img src=./picture/Improving-DeepNeuralNetwork-week2-7-2.jpg width="800" />

<a name="local-optima"><h4>2.7. 解决局部最优问题 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week2-local-optima-problem.png width="800" />

当特征空间为低维时，的确容易出现多个不同的局部最优，但是在高维度的空间中，如果梯度为0，更有可能碰到鞍点，而不是碰到局部最优。

<img src=./picture/Improving-DeepNeuralNetwork-week2-8-1.jpg width="800" />

<a name="week3"><h3>第三周：超参数调试、Batch正则化和程序框架 [<sup>目录</sup>](#content)</h3></a>

<a name="debug-hyperparameters"><h4>3.1. 超参数调试 [<sup>目录</sup>](#content)</h4></a>

<a name="search-hyperparameters-space"><h4>3.1.1. 搜索超参数空间 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week3-1.jpg width="800" />

<a name="chose-proper-scale"><h4>3.1.2. 为超参数选择合适的搜索尺牍（范围） [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week3-2.jpg width="800" />

<a name="hyperparameters-training-strategies"><h4>3.1.3. 超参数训练的两大策略 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week3-hyperparameters-training-strategies.png width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week3-3-1.jpg width="800" />

<a name="batch-normalization"><h4>3.2. Batch正则化 [<sup>目录</sup>](#content)</h4></a>

<a name="normalizing-activation-in-a-network"><h4>3.2.1. 正则化网络的激活函数 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week3-3-2.jpg width="800" />

<a name="implementing-batch-norm"><h4>3.2.2. 实施Batch归一化 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week3-4.jpg width="800" />

<a name="apply-batch-norm-in-nn"><h4>3.2.3. 将Batch Norm 拟合进神经网络 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week3-5.jpg width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week3-6-1.jpg width="800" />

<a name="understand-batch-norm"><h4>3.2.4. 理解Batch Norm的作用 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week3-why-batch-norm-work.png width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week3-6-2.jpg width="800" />

<img src=./picture/Improving-DeepNeuralNetwork-week3-7-1.jpg width="800" />

<a name="batch-norm-during-train-or-test-time"><h4>3.2.5. Batch Norm在训练与测试中的不同操作 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week3-7-2.jpg width="800" />

<a name="softmax-regression"><h4>3.3. softmax回归 [<sup>目录</sup>](#content)</h4></a>

<img src=./picture/Improving-DeepNeuralNetwork-week3-8.jpg width="800" />

softmax神经网络训练

<img src=./picture/Improving-DeepNeuralNetwork-week3-9-1.jpg width="800" />
