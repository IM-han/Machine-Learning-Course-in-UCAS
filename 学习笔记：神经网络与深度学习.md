<a name="content">目录</a>

[学习笔记：神经网络与深度学习](#title)
- [摘要](#abstract)
- [第一周：深度学习概论](#introduction)
	- [什么是神经网络](#neural-network)
	- [用神经网络进行监督学习](#supervised-learning-with-neural-network)
	- [深度学习兴起的原因](#reason-of-boom)
- [第二周：神经网络基础](#basic-neural-network)
	- [二分分类及符号约定](#binary-classification-and-sign)
	- [logistic回归](#logistic-regression)
	- [logistic回归损失函数](#loss-function)
	- [损失函数优化方法：梯度下降法](#gradiant-descent)
	- [导数基础知识](#basic-deviation)
	- [计数图与计数图导数](#computation-graph-deviation)
	- [logistic 回归中的梯度下降法](#gradient-descent-on-logistic-regression)
		- [只考虑一个样本时](#single-sample)
		- [考虑m个样本时](#m-samples)
	- [向量化技术](#vectorization)
	- [向量化 logistic 回归](#vectorizing-logistic-regression)
	- [Python/numpy 向量的说明](#vector-in-numpy)
	- [logistic 回归成本函数证明](#origin-of-cost-function-in-logistic-regression)
- [第三周：浅层神经网络](#shallow-neural-network)
	- [神经网络概览](#overview-neural-network)
	- [神经网络表示](#presentation-neural-network)
	- [计算神经网络的输出](#compute-output-of-neural-network)
	- [多个例子的向量化](#vectorizing-multiple-examples)
	- [激活函数](#active-function)
	- [为什么需要非线性激活函数](#reason-of-using-nonliner-active-function)
	- [激活函数的导数](#deviation-of-active-function)
	- [神经网络的梯度下降法](#gradient-descent-for-neural-network)
	- [随机初始化](#random-initiation)
- [第四周：深层神经网络](#deep-neural-network)
	- [深层神经网络与符号约定](#deep-neural-network-notation)
	- [深层网络中的前向传播](#forward-propagation-in-deep-neural-network)
	- [使用深层网络的原因](#reason-of-using-deep-neural-network)
	- [前向和反向传播模块](#forward-backward-propagation-blocks)
	- [超参数](#hyperparameters)
- [补充笔记](#supplimentary)
    - [*1. logistic regression的梯度/导数推导](#derivation-of-lr-gradient)
    - [*2. tanh函数表达式的由来](#origin-of-tanh-function)

<h1 name="title">学习笔记：神经网络与深度学习</h1>

<p align="center"><img src=/picture/deeplearning.ai.logo.png width="400" /></p>

<a name="abstract"><h3>摘要 [<sup>目录</sup>](#content)</h3></a>

<img src=./picture/NeuralNetwork&DeepLearning-summary-1.png width="800" />

<img src=./picture/NeuralNetwork&DeepLearning-summary-2.png width="800" />

<img src=./picture/NeuralNetwork&DeepLearning-summary-3.png width="800" />

<a name="introduction"><h3>第一周：深度学习概论 [<sup>目录</sup>](#content)</h3></a>

<a name="neural-network"><h4>什么是神经网络 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week1-1.jpg width="800" />

<img src=/picture/NeuralNetwork&DeepLearning-Week1-2-1.jpg width="800" />

<a name="supervised-learning-with-neural-network"><h4>用神经网络进行监督学习 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week1-2-2.jpg width="800" />

<img src=/picture/NeuralNetwork&DeepLearning-Week1-3-1.jpg width="800" />

<a name="reason-of-boom"><h4>深度学习兴起的原因 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week1-curve.png width="800" />

<img src=/picture/NeuralNetwork&DeepLearning-Week1-3-2.jpg width="800" />

<a name="basic-neural-network"><h3>第二周：神经网络基础 [<sup>目录</sup>](#content)</h3></a>

<a name="binary-classification-and-sign"><h4>二分分类及符号约定 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week2-1-1.jpg width="800" />

<a name="logistic-regression"><h4>logistic回归 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week2-1-2.jpg width="800" />

<a name="loss-function"><h4>logistic回归损失函数 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week2-2.jpg width="800" />

<img src=/picture/NeuralNetwork&DeepLearning-Week2-3-1.jpg width="800" />

<a name="gradiant-descent"><h4>损失函数优化方法：梯度下降法 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week2-3-2.jpg width="800" />

<a name="basic-deviation"><h4>导数基础知识 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=/picture/NeuralNetwork&DeepLearning-Week2-deviation-formula.png width="400" /></p>

<a name="computation-graph-deviation"><h4>计数图与计数图导数 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week2-3-3.jpg width="800" />

微积分链式法则

<img src=/picture/NeuralNetwork&DeepLearning-Week2-4-1.jpg width="800" />

<a name="gradient-descent-on-logistic-regression"><h4>logistic回归中的梯度下降法 [<sup>目录</sup>](#content)</h4></a>

<a name="single-sample">只考虑一个样本时</a>

<img src=/picture/NeuralNetwork&DeepLearning-Week2-4-2.jpg width="800" />

<a name="m-samples">考虑m个样本时</a>

<img src=/picture/NeuralNetwork&DeepLearning-Week2-5.jpg width="800" />

<a name="vectorization"><h4>向量化技术 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week2-6-1.jpg width="800" />

用向量化方法优化logistic回归中的第二个for循环

<table>
<tr>
	<td><img src=/picture/NeuralNetwork&DeepLearning-Week2-gradientDescent-code-non-vectorized.jpg width="400" /></td>
	<td><img src=/picture/NeuralNetwork&DeepLearning-Week2-gradientDescent-code-vectorized.jpg width="400" /></td>
</tr>
</table>

<a name="vectorizing-logistic-regression"><h4>向量化 logistic 回归 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week2-6-2.jpg width="800" />

<img src=/picture/NeuralNetwork&DeepLearning-Week2-7-1.jpg width="800" />

<table>
<tr>
	<td><img src=/picture/NeuralNetwork&DeepLearning-Week2-gradientDescent-code-non-vectorized.jpg width="500" /></td>
	<td><img src=/picture/NeuralNetwork&DeepLearning-Week2-bp-vectorized.jpg width="200" /></td>
</tr>
</table>

<a name="vector-in-numpy"><h4>Python/numpy 向量的说明 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week2-7-2.jpg width="800" />

<img src=/picture/NeuralNetwork&DeepLearning-Week2-8-1.jpg width="800" />

<a name="origin-of-cost-function-in-logistic-regression"><h4>logistic 回归成本函数证明 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week2-8-2.jpg width="800" />

<a name="shallow-neural-network"><h3>第三周：浅层神经网络 [<sup>目录</sup>](#content)</h3></a>

<a name="overview-neural-network"><h4>神经网络概览 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week3-1-1.jpg width="800" />

<a name="presentation-neural-network"><h4>神经网络表示 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week3-1-2.jpg width="800" />

<img src=/picture/NeuralNetwork&DeepLearning-Week3-2-1.jpg width="800" />

<a name="compute-output-of-neural-network"><h4>计算神经网络的输出 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week3-logistic-unit-and-stack.png width="800" />

<img src=/picture/NeuralNetwork&DeepLearning-Week3-compute4hiddenNode.png width="800" />

<img src=/picture/NeuralNetwork&DeepLearning-Week3-2-2.jpg width="800" />

<a name="vectorizing-multiple-examples"><h4>多个例子的向量化 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week3-3-1.jpg width="800" />

<a name="active-function"><h4>激活函数 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week3-3-2.jpg width="800" />

<img src=/picture/NeuralNetwork&DeepLearning-Week3-4.jpg width="800" />

<img src=/picture/NeuralNetwork&DeepLearning-Week3-5-1.jpg width="800" />

<a name="reason-of-using-nonliner-active-function"><h4>为什么需要非线性激活函数 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week3-5-2.jpg width="800" />

只有一个地方可以使用线性激活函数，就是如果你要机器学习的是回归问题，在输出层可以使用线性激活函数。

<a name="deviation-of-active-function"><h4>激活函数的导数 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week3-6.jpg width="800" />

<a name="gradient-descent-for-neural-network"><h4>神经网络的梯度下降法 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=/picture/NeuralNetwork&DeepLearning-Week3-Gradient-descent.jpg width="500" /></p>

<table>
<tr>
	<td><img src=/picture/NeuralNetwork&DeepLearning-Week3-Forward-Propagation.jpg width="300" /></td>
	<td><img src=/picture/NeuralNetwork&DeepLearning-Week3-Back-Propagation.jpg width="500" /></td>
</tr>
</table>

<a name="random-initiation"><h4>随机初始化 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week3-8.jpg width="800" />

<a name="deep-neural-network"><h3>第四周：深层神经网络 [<sup>目录</sup>](#content)</h3></a>

<a name="deep-neural-network-notation"><h4>深层神经网络与符号约定 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week4-1-1.jpg width="800" />

<a name="forward-propagation-in-deep-neural-network"><h4>深层网络中的前向传播 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week4-1-2.jpg width="800" />

<a name="reason-of-using-deep-neural-network"><h4>使用深层网络的原因 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week4-valid-of-deep-neural-network.png width="800" />

<img src=/picture/NeuralNetwork&DeepLearning-Week4-2.jpg width="800" />

<a name="forward-backward-propagation-blocks"><h4>前向和反向传播模块 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week4-3.jpg width="800" />

<img src=/picture/NeuralNetwork&DeepLearning-Week4-4-1.jpg width="800" />

<a name="hyperparameters"><h4>超参数 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-Week4-4-2.jpg width="800" />

<a name="supplimentary"><h3>补充笔记 [<sup>目录</sup>](#content)</h3></a>

<a name="derivation-of-lr-gradient"><h4>*1. logistic regression的梯度/导数推导 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-supplimentary-1.jpg width="800" />

<a name="origin-of-tanh-function"><h4>*2. tanh函数表达式的由来 [<sup>目录</sup>](#content)</h4></a>

<img src=/picture/NeuralNetwork&DeepLearning-supplimentary-2.jpg width="800" />
